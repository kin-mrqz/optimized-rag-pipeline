{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058c0fc4-01ea-41de-9dc3-2fc9d755e75c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (0.1.7)\n",
      "Requirement already satisfied: langchainhub in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (0.1.21)\n",
      "Requirement already satisfied: chromadb in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (1.0.13)\n",
      "Requirement already satisfied: langchain in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (0.2.17)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from langchain_community) (2.0.38)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from langchain_community) (3.11.18)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from langchain_community) (0.2.43)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from langchain_community) (0.1.147)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from langchain) (1.10.22)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (4.6.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.24.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from langchain-openai) (1.88.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from langchainhub) (2.32.4.20250611)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (5.3.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (1.73.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.20.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.12.23)\n",
      "Requirement already satisfied: protobuf in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4 sentence-transformers tqdm langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94658a9f-9156-4432-bbf7-5547e9a1c654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import requests\n",
    "import bs4\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain.llms.base import LLM\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader, GoogleDriveLoader, UnstructuredWordDocumentLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI  # Uses OpenAI-compatible API\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb40f71-d897-4055-a8eb-d3e23824dbfc",
   "metadata": {},
   "source": [
    "**Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb38446-6146-41f6-84d4-7798de8345d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pdf contents\n",
    "def load_pdf(path: str):\n",
    "    loader = PyPDFLoader(path)\n",
    "    return loader.load()\n",
    "\n",
    "def load_word(path: str):\n",
    "    return UnstructuredWordDocumentLoader(path).load()\n",
    "    \n",
    "# loading website contents\n",
    "def load_web(path: str):\n",
    "    loader = WebBaseLoader(\n",
    "        web_path = (path,),\n",
    "        bs_kwargs = dict(\n",
    "          parse_only = bs4.SoupStrainer(\n",
    "              class_ = (\"post-content\", \"post-title\", \"post-header\") # depending on CSS class\n",
    "          )  \n",
    "        ),\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "# loading google doc contents (see below)\n",
    "\n",
    "def normalize(text):\n",
    "    return re.sub(r'[^a-z]', '', str(text).lower())\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def extract_json_from_llm_output(output: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts and parses a JSON object from LLM output that may include Markdown formatting.\n",
    "    Handles triple backticks, optional language labels, and excessive whitespace.\n",
    "    \"\"\"\n",
    "    output = output.strip()\n",
    "\n",
    "    # Match content between ```json ... ``` or just ``` ... ```\n",
    "    match = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", output, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "    else:\n",
    "        json_str = output\n",
    "\n",
    "    return json.loads(json_str)\n",
    "\n",
    "def get_doc_id(url):\n",
    "    return url.split(\"/\")[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ecbdfe-e90d-4107-b067-5d0b1c906c38",
   "metadata": {},
   "source": [
    "OpenAI embeddings don't work in HK (even with VPN). Hence, HuggingFace embedding model was used. Also feel free to test out API calls from DeepSeep or replace it with your choice of LLM for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9351f27e-9f8a-4ee3-ba55-f1689883be5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17484\\3436719414.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embd = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "C:\\Users\\User\\rag-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embd = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# change to bedrock llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=\"https://api.deepseek.com/v1\",\n",
    "    openai_api_key=\"sk-ea1868b36aa34a36be9a223e75c1c63c\", \n",
    "    model=\"deepseek-chat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee50444-8d61-458e-bc20-5bc1a007ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load spacy models\n",
    "nlp_textcat = spacy.load(\"textcat_model\")\n",
    "nlp_ner = spacy.load(\"ner_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b52757-b688-41af-ae35-fbfa1f376195",
   "metadata": {},
   "source": [
    "**Intermediate Step: Preparing Google Cloud API**\n",
    "- for loading contents of Google Docs\n",
    "- you can also try loading content using **load_pdf()** and **load_web**\n",
    "- the code below uses my Google Cloud credentials included in this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b249a1e4-5085-430d-97bd-b00cd1fc87b4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-auth in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (2.40.3)\n",
      "Collecting google-auth-oauthlib\n",
      "  Using cached google_auth_oauthlib-1.2.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.174.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from google-auth) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from google-auth) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from google-auth) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from google-auth-oauthlib) (2.0.0)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 (from google-api-python-client)\n",
      "  Using cached google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Using cached uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.5)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client)\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.4.26)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.3.1)\n",
      "Using cached google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\n",
      "Downloading google_api_python_client-2.174.0-py3-none-any.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.3/13.7 MB 6.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.9/13.7 MB 7.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.5/13.7 MB 8.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.9/13.7 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.6/13.7 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 11.9 MB/s eta 0:00:00\n",
      "Using cached google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: uritemplate, proto-plus, httplib2, google-auth-oauthlib, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [proto-plus]\n",
      "   ----- ---------------------------------- 1/7 [proto-plus]\n",
      "   ----- ---------------------------------- 1/7 [proto-plus]\n",
      "   ----- ---------------------------------- 1/7 [proto-plus]\n",
      "   ----------- ---------------------------- 2/7 [httplib2]\n",
      "   ----------- ---------------------------- 2/7 [httplib2]\n",
      "   ----------------- ---------------------- 3/7 [google-auth-oauthlib]\n",
      "   ----------------- ---------------------- 3/7 [google-auth-oauthlib]\n",
      "   ---------------------------- ----------- 5/7 [google-api-core]\n",
      "   ---------------------------- ----------- 5/7 [google-api-core]\n",
      "   ---------------------------- ----------- 5/7 [google-api-core]\n",
      "   ---------------------------- ----------- 5/7 [google-api-core]\n",
      "   ---------------------------- ----------- 5/7 [google-api-core]\n",
      "   ---------------------------- ----------- 5/7 [google-api-core]\n",
      "   ---------------------------- ----------- 5/7 [google-api-core]\n",
      "   ---------------------------- ----------- 5/7 [google-api-core]\n",
      "   ---------------------------- ----------- 5/7 [google-api-core]\n",
      "   ---------------------------- ----------- 5/7 [google-api-core]\n",
      "   ---------------------------------- ----- 6/7 [google-api-python-client]\n",
      "   ---------------------------------- ----- 6/7 [google-api-python-client]\n",
      "   ---------------------------------- ----- 6/7 [google-api-python-client]\n",
      "   ---------------------------------- ----- 6/7 [google-api-python-client]\n",
      "   ---------------------------------- ----- 6/7 [google-api-python-client]\n",
      "   ---------------------------------- ----- 6/7 [google-api-python-client]\n",
      "   ---------------------------------- ----- 6/7 [google-api-python-client]\n",
      "   ---------------------------------- ----- 6/7 [google-api-python-client]\n",
      "   ---------------------------------- ----- 6/7 [google-api-python-client]\n",
      "   ---------------------------------- ----- 6/7 [google-api-python-client]\n",
      "   ---------------------------------- ----- 6/7 [google-api-python-client]\n",
      "   ---------------------------------- ----- 6/7 [google-api-python-client]\n",
      "   ---------------------------------------- 7/7 [google-api-python-client]\n",
      "\n",
      "Successfully installed google-api-core-2.25.1 google-api-python-client-2.174.0 google-auth-httplib2-0.2.0 google-auth-oauthlib-1.2.2 httplib2-0.22.0 proto-plus-1.26.1 uritemplate-4.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade google-auth google-auth-oauthlib google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04c57334-5b2e-4a03-8472-1f15330825b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=808603111555-e7db6vhbucu4hj1ovgmd01ikvsnqn8ul.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A52225%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.readonly&state=urGcMPuC2LXklegz4rE1qcXPpuFTuJ&access_type=offline\n",
      "✅ token.json generated successfully.\n"
     ]
    }
   ],
   "source": [
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "\n",
    "# Correct scope\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.readonly']\n",
    "\n",
    "def generate_token():\n",
    "    flow = InstalledAppFlow.from_client_secrets_file('credentials_google.json', SCOPES)\n",
    "    creds = flow.run_local_server(port=0)\n",
    "\n",
    "    with open('token.json', 'w') as token:\n",
    "        token.write(creds.to_json())\n",
    "\n",
    "    print(\"✅ token.json generated successfully.\")\n",
    "\n",
    "generate_token()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2009ded1-d845-4a7c-b0b1-d0e360bc2ddd",
   "metadata": {},
   "source": [
    "doc_links contains the Google Docs from the Sommelier Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93df9f04-79ca-459b-b80d-e9a5e11f2944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17484\\1336585303.py:18: LangChainDeprecationWarning: The class `GoogleDriveLoader` was deprecated in LangChain 0.0.32 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-google-community package and should be used instead. To use it run `pip install -U :class:`~langchain-google-community` and import as `from :class:`~langchain_google_community import GoogleDriveLoader``.\n",
      "  g_docs = GoogleDriveLoader(\n"
     ]
    }
   ],
   "source": [
    "docs_links = [\n",
    "    \"https://docs.google.com/document/d/1MvX9CTrVcoWg7WLAscq2MmnhTIrR0hZIGkpJMqhgflo/edit?tab=t.0\",\n",
    "    \"https://docs.google.com/document/d/1NcV9_JGjMfA4WlihW3vNTduy24NBWdy1RWXrA2W0BIk/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/193rx2Rh6u-Ud40k-rgnqSQs-94SvHdeXPrPxOWK59X0/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/1vRDsn5o5mdymOEJ_O0tS4wcOjsAjt_2mLZqFfvgDUOs/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/1JceLBII727AZzSrDFfdGthJ1G4PhCDsA8sEm_dQMVr0/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/1yonU4qcysNkgd0BvbFmeIW9NF2ARErRJVW8QZynJyvM/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/1bq2AE1Jy6cQFt1xgjqtkof12Lw6F6fujqTlN1nZnh0A/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/1i-OcQeo7XOG83gS2ay2u0SLMWs4f8FG0JE_7l87qJkw/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/1PyZE8v3S3aUY66lFn97q0vaXDHc60lyso2oUFP0htjY/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/1Dudd7-6yl_UQrxfGa3MJZlKOfzHQdqWg0fb8Z9RzBec/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/1cjhrhCccuwiIh0ujj8QeamJ2JHhI6CjPmO84t1DSRZ0/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/1ESlfU6v8jseFlllZb3eaUeCJt69EIMsZiyMrDac-wX8/edit?usp=sharing\",\n",
    "    \"https://docs.google.com/document/d/1xQhAkC3oP2cb262EjaHCV6CxEgeEGUsKrC8pH2p6RiY/edit?usp=sharing\"\n",
    "    \n",
    "]\n",
    "\n",
    "g_docs = GoogleDriveLoader(\n",
    "    document_ids = [get_doc_id(i) for i in docs_links],\n",
    "    credentials_path = \"credentials_google.json\",\n",
    "    token_path = \"token.json\"\n",
    ").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9dba0-a5b4-44db-8266-0f3e324d0674",
   "metadata": {},
   "source": [
    "Chroma seems to be a convenient alternative, as a chroma vectorstore can be converted directly into a retriever that returns k relevant documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8c95311-858e-4c5d-b856-b7b9ff4ba5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading sample pdf document\n",
    "path = \"wine_food_pairing_knowledge.pdf\"\n",
    "word = load_pdf(path)\n",
    "g_docs = g_docs + word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b795521-0dc5-49c4-bc4e-c527627787e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 300, \n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(g_docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = splits,\n",
    "    embedding = embd\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d77ae9-9457-4958-8bfd-25f627b52ac5",
   "metadata": {},
   "source": [
    "**Preparing Pydantic Schemas and LLM Modelfiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06316cfb-4c7d-4c0a-983e-c503104dd92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# define simple schema as template, map doc.ent if present else None\n",
    "class WineMetadata(BaseModel):\n",
    "    \"\"\"Schema for metadata filters\"\"\"\n",
    "    wine_name: Optional[str] = Field(default = None, description = \"Specific wine name mentioned in query\")\n",
    "    max_price: Optional[float] = Field(default = None, description = \"Specific max price mentioned in the query\")\n",
    "    min_price: Optional[float] = Field(default = None, description = \"Specific min price mentioned in the query\")\n",
    "    wine_type: Optional[str] = Field(default = None, description = \"Specific wine type (e.g., white, red, sparkling, etc.)\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b3951ec-46df-4018-8cc7-1be920c9ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for generating taste profiles\n",
    "\n",
    "system = \"\"\"You are a master-level sommelier and wine data expert.\n",
    "Your task is to create a hypothetical, ideal wine profile in the form of structured metadata based on a user's preference, question, or context.\n",
    "\n",
    "This wine does not have to exist — it should represent the best possible match for what the user is looking for.\n",
    "\n",
    "You must output a plausible, detailed JSON object that aligns with the schema below.\n",
    "\n",
    "Do not refer back to the user query. Do not explain. Only output the JSON.\n",
    "\n",
    "{\n",
    "  \"wine_name\": \"str\",              // Specific wine name mentioned\n",
    "  \"winemaker\": \"str\",              // Name of the winemaker or producer\n",
    "  \"vintage\": \"int\",                // Vintage year (e.g. 2015)\n",
    "  \"country\": \"str\",                // Country of origin\n",
    "  \"region\": \"str\",                 // Region or appellation\n",
    "  \"wine_type\": \"str\",              // Type of wine (e.g. red, white, rosé, sparkling)\n",
    "  \"wine_grapes\": \"str\",            // Grape variety or blend (e.g. Merlot, Syrah)\n",
    "  \"occasion\": \"str\",               // Occasion suitability (e.g. \"wedding\", \"gift\", \"everyday\")\n",
    "  \"body\": \"str\",                   // Body type (e.g. \"light\", \"full-bodied\")\n",
    "  \"acidity\": \"str\",                // Acidity level (e.g. \"crisp\", \"low\")\n",
    "  \"alcohol\": \"float\",              // Alcohol content (as percentage, e.g. 13.5)\n",
    "  \"fruitiness\": \"str\",             // Level of fruitiness (e.g. \"dry\", \"juicy\")\n",
    "  \"minerality\": \"str\",             // Presence of mineral notes (e.g. \"chalky\", \"flinty\")\n",
    "  \"sweetness\": \"str\",              // Sweetness level (e.g. \"dry\", \"semi-sweet\", \"sweet\")\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50d4b3ff-daa1-44dd-b7d1-1f08f38275f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELD_MAP = {\n",
    "    \"wine_name\": \"Product Name\",\n",
    "    \"min_price\": \"WS Retail Price\",\n",
    "    \"max_price\": \"WS Retail Price\",\n",
    "    \"wine_type\": \"Wine Type\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01f672-e880-49cd-986f-60930b18919d",
   "metadata": {},
   "source": [
    "**More helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27dcfc86-f16d-4a6f-8a4a-ef84714bdc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwatch\n",
    "import time\n",
    "import random\n",
    "from statistics import quantiles\n",
    "from functools import wraps\n",
    "\n",
    "def timed(log_times):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end = time.time()\n",
    "            log_times[func.__name__] = round(end - start, 4)\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "log_times = {}\n",
    "\n",
    "# identify if the intent is to recommend food or wine\n",
    "def recommend_wine(query:str):\n",
    "    category = nlp_textcat(query)\n",
    "    recommend_wine = category.cats[\"recommend_wine\"]\n",
    "    recommend_food = category.cats[\"recommend_food\"]\n",
    "    if recommend_wine > recommend_food:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_price(wine, price_field = \"price\"):\n",
    "    try:\n",
    "        return float(wine.get(price_field, 0))\n",
    "    except (ValueError, TypeError):\n",
    "        return 0.0\n",
    "\n",
    "def sample_quartiles(wines, price_field=\"price\", k_per_quartile=10):\n",
    "    prices = [get_price(w, price_field) for w in wines]\n",
    "    if not prices:\n",
    "        return []\n",
    "\n",
    "    q1, q2, q3 = quantiles(prices, n=4)\n",
    "\n",
    "    buckets = {\n",
    "        \"Q1\": [w for w in wines if get_price(w, price_field <= q1)],\n",
    "        \"Q2\": [w for w in wines if q1 < get_price(w, price_field) <= q2],\n",
    "        \"Q3\": [w for w in wines if q2 < get_price(w, price_field) <= q3],\n",
    "        \"Q4\": [w for w in wines if q3 < get_price(w, price_field)]\n",
    "    }\n",
    "\n",
    "    sampled = []\n",
    "    for group in buckets.values():\n",
    "        if group:\n",
    "            sampled.extend(random.sample(group, min(k_per_quartile, len(group))))\n",
    "    return sampled                         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a8f126f-a5e3-44bc-8a3c-8f7a263a93bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_price': 300.0, 'min_price': None, 'wine_name': None, 'wine_type': 'sparkling'}\n",
      "sparkling not in white\n",
      "sparkling not in red\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in red\n",
      "sparkling not in red\n",
      "sparkling not in white\n",
      "sparkling not in red\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in red\n",
      "sparkling not in white\n",
      "sparkling not in red\n",
      "sparkling not in red\n",
      "sparkling not in red\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in red\n",
      "sparkling not in red\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in fortified\n",
      "sparkling not in white\n",
      "sparkling not in red\n",
      "sparkling not in ros\n",
      "sparkling not in red\n",
      "sparkling not in white\n",
      "sparkling not in red\n",
      "sparkling not in red\n",
      "sparkling not in white\n",
      "sparkling not in ros\n",
      "sparkling not in white\n",
      "sparkling not in red\n",
      "sparkling not in red\n",
      "sparkling not in white\n",
      "sparkling not in red\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in red\n",
      "sparkling not in red\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in white\n",
      "sparkling not in red\n",
      "sparkling not in red\n"
     ]
    }
   ],
   "source": [
    "question = \"Recommend a sparkling wine under 300 HKD that pairs well with grilled lamb and comes from Spain\"\n",
    "start_main = time.time()\n",
    "parsed_query = query_analyzer(question)\n",
    "filtered = filter_wines(data=wine_json, model_instance=parsed_query, field_map=FIELD_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5465c0c-6f81-45d3-987b-dd7aab55ff1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ae98a79-0efc-47bd-a4fb-bf6d776dcfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "@timed(log_times)\n",
    "def query_analyzer(query: str):\n",
    "    \"\"\" Extracts structured wine metadata from a natural language query\n",
    "    using a spaCy NER model and returns it as a WineMetadata object.\"\"\"\n",
    "    \n",
    "    if recommend_wine(query):\n",
    "        doc = nlp_ner(query)\n",
    "        all_labels = nlp_ner.get_pipe(\"ner\").labels\n",
    "        result = {label: None for label in all_labels}\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if (ent.label_ == \"min_price\") or (ent.label_ == \"max_price\"):\n",
    "                result[ent.label_] = float(ent.text)\n",
    "            elif (ent.label_ == \"wine_type\"):\n",
    "                if ent.text in [\"red\", \"white\", \"sparkling\"]:\n",
    "                    result[ent.label_] = ent.text\n",
    "                else:\n",
    "                    result[ent.label_] = None \n",
    "            else:\n",
    "                result[ent.label_] = ent.text\n",
    "            \n",
    "        print(result)\n",
    "        return WineMetadata(**result)\n",
    "\n",
    "@timed(log_times)\n",
    "def filter_wines(data, model_instance, field_map, max_results=20):\n",
    "    \"\"\"\n",
    "    Given metadata filters constructed from user query using LLM,\n",
    "    returns first n wine profiles that match. Also includes fallback logic\n",
    "    \"\"\"\n",
    "    filters = model_instance.model_dump(exclude_none=True)\n",
    "    results = []\n",
    "\n",
    "    # identify filter state\n",
    "    has_filters = bool(filters)\n",
    "    has_price = \"min_price\" in filters or \"max_price\" in filters\n",
    "    has_type = \"wine_type\" in filters\n",
    "    \n",
    "    # case 1: have both filters\n",
    "    if has_filters and ( len(filters) > 1 or has_price ):\n",
    "        for wine in data:\n",
    "            match = True\n",
    "            for key, value in filters.items():\n",
    "    \n",
    "                if key not in field_map:\n",
    "                    print(f\"{key} not in field map\")\n",
    "                    continue\n",
    "                    \n",
    "                if key in (\"min_price\", \"max_price\"):\n",
    "                    wine_price = get_price(wine, \"price\")\n",
    "                    if key == \"min_price\" and wine_price < value:\n",
    "                        match = False\n",
    "                        break\n",
    "                    if key == \"max_price\" and wine_price > value:\n",
    "                        match = False\n",
    "                        break\n",
    "    \n",
    "                else:\n",
    "                    field = field_map.get(key)\n",
    "                    if field not in wine:\n",
    "                        # print(f\"{field} not in wine\")\n",
    "                        match = False\n",
    "                        break\n",
    "                    wine_val = normalize(wine[field])\n",
    "                    query_val = normalize(value)\n",
    "                    if query_val not in wine_val:\n",
    "                        # print(f\"{query_val} not in {wine_val}\")\n",
    "                        match = False\n",
    "                        break\n",
    "    \n",
    "            if match:\n",
    "                results.append(wine)\n",
    "                if len(results)== max_results:\n",
    "                    break\n",
    "    \n",
    "        return results\n",
    "\n",
    "    # case 2: filter only has wine_type (no price)\n",
    "    elif has_type and not has_price:\n",
    "        wine_type = normalize(filters[\"wine_type\"])\n",
    "        type_filtered = [w for w in data if normalize(w.get(field_map[\"wine_type\"], \"\")) == wine_type]\n",
    "        return sample_quartiles(type_filtered, price_field=price_field, k_per_quartile=4)\n",
    "\n",
    "    else:\n",
    "        type_buckets = {\n",
    "            \"red\": [],\n",
    "            \"white\": [],\n",
    "            \"sparkling\": []\n",
    "        }\n",
    "        for w in data:\n",
    "            wt = normalize(w.get(field_map.get(\"wine_type\", \"wine_type\"), \"\"))\n",
    "            if wt in type_buckets:\n",
    "                type_buckets[wt].append(w)\n",
    "\n",
    "        sampled = []\n",
    "        sampled += sample_quartiles(type_buckets[\"red\"], price_field, k_per_quartile=10)\n",
    "        sampled += sample_quartiles(type_buckets[\"white\"], price_field, k_per_quartile=10)\n",
    "        sampled += sample_quartiles(type_buckets[\"sparkling\"], price_field, k_per_quartile=10)\n",
    "\n",
    "        return sampled[:max_results]\n",
    "\n",
    "\n",
    "@timed(log_times)\n",
    "def create_taste_profile(query: str):\n",
    "    \"\"\"\n",
    "    Generates a hypothetical document embedding (HyDE) of \n",
    "    the ideal taste profile based on user query.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=system),\n",
    "        HumanMessage(content=query)\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    print(response.content + \"\\n\")\n",
    "    \n",
    "    return response.content         \n",
    "\n",
    "    \n",
    "@timed(log_times)\n",
    "def generate_recommendations(filtered, profile, top_k=5):\n",
    "    \"\"\"\n",
    "    Performs similarity search between embeddings of (a) each of the filtered wines\n",
    "    and (b) enriched taste profile from query, if present otherwise randomly \n",
    "    sample 3 wines (cheapest -> middle -> most expensive)\n",
    "    \"\"\"\n",
    "    if len(filtered) == 0:\n",
    "        return None\n",
    "        \n",
    "    if len(filtered) < top_k:\n",
    "        return filtered\n",
    "        \n",
    "    if profile:\n",
    "        embedded_wines = []\n",
    "        for wine in filtered:\n",
    "            content = \"\\n\".join(f\"{key}: {value}\" for key, value in wine.items() if value)\n",
    "            wine_embed = embd.embed_query(content)\n",
    "            embedded_wines.append((wine, wine_embed))\n",
    "\n",
    "        profile_embed = embd.embed_query(profile)\n",
    "        scored_embed = [(wine, cosine_similarity(profile_embed, wine_embed)) for wine, wine_embed in embedded_wines]\n",
    "        top_wines = sorted(scored_embed, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        return [wine for wine, _ in top_wines]\n",
    "\n",
    "    else:\n",
    "        sorted_wines = sorted(filtered, key=lambda x: float(x.WS_Retail_Price))\n",
    "        n = len(sorted_wines)\n",
    "        if n == 0:\n",
    "            return []\n",
    "        step = max(1, n // top_k)\n",
    "        sampled = [random.choice(sorted_wines[i:i+step]) for i in range(0, n, step)][:top_k]\n",
    "\n",
    "        return sampled \n",
    "\n",
    "def ask_ai(question, data, field_map):\n",
    "    start_main = time.time()\n",
    "    parsed_query = query_analyzer(question)\n",
    "    filtered = filter_wines(data=data, model_instance=parsed_query, field_map=field_map)\n",
    "    profile = create_taste_profile(question)\n",
    "    recommendations = generate_recommendations(filtered, profile)\n",
    "\n",
    "    # Print timing summary\n",
    "    total_time = round(time.time() - start_main, 4)\n",
    "    print(\"Timing summary:\")\n",
    "\n",
    "    for name, duration in log_times.items():\n",
    "        print(f\"  {name}: {duration} seconds\")\n",
    "        \n",
    "    print(f\"  Total time: {total_time} seconds\")\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf8b1294-abe1-41b0-be5a-34fd8cd70ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = \"wine_data.csv\"\n",
    "# food_data = \"food_data.csv\"\n",
    "\n",
    "wine_df = pd.read_csv(wine_data)\n",
    "# food_df = pd.read_csv(food_data)\n",
    "\n",
    "wine_json = wine_df.to_dict(orient=\"records\")\n",
    "# food_json = food_df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400d945-5488-49db-b195-b514f979d3ec",
   "metadata": {},
   "source": [
    "### Wine-Food Pairing Tests\n",
    "\n",
    "Questions:\n",
    "1. What wine goes well with spicy Thai green curry with coconut milk?\n",
    "2. Recommend a red wine under 300 HKD that pairs well with grilled lamb and comes from Spain\n",
    "3. Suggest a celebratory wine that works with oysters and has high acidity.\n",
    "4. I'm cooking mushroom risotto and want something medium-bodied and earthy to go with it.\n",
    "5. Pair a bold Napa Cabernet Sauvignon with sushi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db5801fe-8694-41cd-95bd-259c5283b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_price': 300.0, 'min_price': None, 'wine_name': None, 'wine_type': 'red'}\n",
      "```json\n",
      "{\n",
      "  \"wine_name\": \"Tinto Pesquera Crianza\",\n",
      "  \"winemaker\": \"Alejandro Fernández\",\n",
      "  \"vintage\": 2018,\n",
      "  \"country\": \"Spain\",\n",
      "  \"region\": \"Ribera del Duero\",\n",
      "  \"wine_type\": \"red\",\n",
      "  \"wine_grapes\": \"Tempranillo\",\n",
      "  \"occasion\": \"dinner\",\n",
      "  \"body\": \"full-bodied\",\n",
      "  \"acidity\": \"medium\",\n",
      "  \"alcohol\": 14.5,\n",
      "  \"fruitiness\": \"ripe\",\n",
      "  \"minerality\": \"subtle\",\n",
      "  \"sweetness\": \"dry\"\n",
      "}\n",
      "```\n",
      "\n",
      "Timing summary:\n",
      "  query_analyzer: 0.0111 seconds\n",
      "  filter_wines: 0.0 seconds\n",
      "  create_taste_profile: 10.2032 seconds\n",
      "  generate_recommendations: 2.6296 seconds\n",
      "  Total time: 12.8439 seconds\n",
      "Wine 1: Casa del Cervo Brunello di Montalcino DOCG Tuscany 1998\n",
      "Wine 2: Blank Canvas Anthem Vineyard Pinot Noir 2019\n",
      "Wine 3: Blank Canvas Wines Settlement Vineyard Pinot Noir Marlborough 2020\n",
      "Wine 4: Cantina Vignaioli Barbaresco 1989\n",
      "Wine 5: Cantina del Glicine Barbaresco Riserva Speciale Vigna del Marcorino 1985\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Recommend a red wine under 300 HKD that pairs well with grilled lamb and comes from Spain\"\n",
    "recommendation = ask_ai(question=question, data=wine_json, field_map=FIELD_MAP)\n",
    "if recommendation == None:\n",
    "    print(\"No recommendations found\")\n",
    "\n",
    "for j in range(5):\n",
    "    try:\n",
    "        print(f\"Wine {j+1}:\", recommendation[j]['Product Name'])\n",
    "    except:\n",
    "        print(\"No more suggested wines\")\n",
    "        break\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d63cae9-03bf-4836-a7d3-b1e0badf7a1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What wine goes well with spicy Thai green curry with coconut milk?\n",
      "{'max_price': None, 'min_price': None, 'wine_name': None, 'wine_type': 'What'}\n",
      "```json\n",
      "{\n",
      "  \"wine_type\": \"white\",\n",
      "  \"description\": \"Aromatic and slightly sweet with good acidity to balance the spice\",\n",
      "  \"occasion\": \"pairing with spicy Thai green curry\",\n",
      "  \"body\": \"medium-bodied\",\n",
      "  \"acidity\": \"high\",\n",
      "  \"fruitiness\": \"juicy\",\n",
      "  \"sweetness\": \"off-dry\"\n",
      "}\n",
      "```\n",
      "Timing summary:\n",
      "  query_analyzer: 0.004 seconds\n",
      "  filter_wines: 0.0 seconds\n",
      "  create_taste_profile: 7.643 seconds\n",
      "  generate_recommendations: 0.0 seconds\n",
      "  Total time: 7.647 seconds\n",
      "No recommendations found\n",
      "Recommend a red wine under 300 HKD that pairs well with grilled lamb and comes from Spain\n",
      "{'max_price': 300.0, 'min_price': None, 'wine_name': None, 'wine_type': 'red'}\n",
      "```json\n",
      "{\n",
      "  \"wine_type\": \"red\",\n",
      "  \"country\": \"Spain\",\n",
      "  \"occasion\": \"grilled lamb pairing\",\n",
      "  \"price_range\": \"under 300 HKD\"\n",
      "}\n",
      "```\n",
      "Timing summary:\n",
      "  query_analyzer: 0.0027 seconds\n",
      "  filter_wines: 0.0 seconds\n",
      "  create_taste_profile: 6.4517 seconds\n",
      "  generate_recommendations: 2.0201 seconds\n",
      "  Total time: 8.4745 seconds\n",
      "Wine 1: Domaine Jourdan Chinon Rouge Les Trois Quartiers 2018\n",
      "Wine 2: Te Mata Awatea Cabernet Merlot, Hawkes Bay 2019\n",
      "Wine 3: Luis Felipe Edwards LFE 900 Colchagua Valley 2018\n",
      "Wine 4: Aurelia Visinescu Red Artisan 2021\n",
      "Wine 5: La Rioja Alta, Vina Alberdi Reserva 2019\n",
      "\n",
      "\n",
      "Suggest a celebratory wine that works with oysters and has high acidity.\n",
      "{'max_price': None, 'min_price': None, 'wine_name': None, 'wine_type': None}\n",
      "```json\n",
      "{\n",
      "  \"occasion\": \"celebratory\",\n",
      "  \"wine_type\": \"white\",\n",
      "  \"acidity\": \"high\",\n",
      "  \"description\": \"pairs well with oysters\"\n",
      "}\n",
      "```\n",
      "Timing summary:\n",
      "  query_analyzer: 0.016 seconds\n",
      "  filter_wines: 0.0 seconds\n",
      "  create_taste_profile: 7.8796 seconds\n",
      "  generate_recommendations: 1.9953 seconds\n",
      "  Total time: 9.8909 seconds\n",
      "Wine 1: Blank Canvas Anandale Farm Riesling 2023\n",
      "Wine 2: Alain Chavy Puligny Montrachet 2020\n",
      "Wine 3: Angelina Brut Reserve Sparkling Wine 2012\n",
      "Wine 4: Armand Heitz Saint Aubin 1er cru murgers dents de chien 2019\n",
      "Wine 5: Andre Beaufort Non Dose Potion x Tregalli, Champagne 2020\n",
      "\n",
      "\n",
      "I'm cooking mushroom risotto and want something medium-bodied and earthy to go with it.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'model_dump'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(questions)):\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# for i in range(2, 5, 2):\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(questions[i])  \u001b[38;5;66;03m# Correct variable name\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     recommendation = \u001b[43mask_ai\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwine_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFIELD_MAP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m recommendation == \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     14\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo recommendations found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 163\u001b[39m, in \u001b[36mask_ai\u001b[39m\u001b[34m(question, data, field_map)\u001b[39m\n\u001b[32m    161\u001b[39m start_main = time.time()\n\u001b[32m    162\u001b[39m parsed_query = query_analyzer(question)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m filtered = \u001b[43mfilter_wines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m profile = create_taste_profile(question)\n\u001b[32m    165\u001b[39m recommendations = generate_recommendations(filtered, profile)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtimed.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m      9\u001b[39m     start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     end = time.time()\n\u001b[32m     12\u001b[39m     log_times[func.\u001b[34m__name__\u001b[39m] = \u001b[38;5;28mround\u001b[39m(end - start, \u001b[32m4\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mfilter_wines\u001b[39m\u001b[34m(data, model_instance, field_map, max_results)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;129m@timed\u001b[39m(log_times)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfilter_wines\u001b[39m(data, model_instance, field_map, max_results=\u001b[32m20\u001b[39m):\n\u001b[32m     33\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m    Given metadata filters constructed from user query using LLM,\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33;03m    returns first n wine profiles that match. \u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     filters = \u001b[43mmodel_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     38\u001b[39m     results = []\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m wine \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'model_dump'"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "\"What wine goes well with spicy Thai green curry with coconut milk?\",\n",
    "\"Recommend a red wine under 300 HKD that pairs well with grilled lamb and comes from Spain\",\n",
    "\"Suggest a celebratory wine that works with oysters and has high acidity.\",\n",
    "\"I'm cooking mushroom risotto and want something medium-bodied and earthy to go with it.\",\n",
    "\"Pair a bold Napa Cabernet Sauvignon with sushi.\",\n",
    "]\n",
    "\n",
    "for i in range(len(questions)):\n",
    "# for i in range(2, 5, 2):\n",
    "    print(questions[i])  # Correct variable name\n",
    "    recommendation = ask_ai(question=questions[i], data=wine_json, field_map=FIELD_MAP)\n",
    "    if recommendation == None:\n",
    "        print(\"No recommendations found\")\n",
    "        continue\n",
    "\n",
    "    for j in range(5):\n",
    "        try:\n",
    "            print(f\"Wine {j+1}:\", recommendation[j]['Product Name'])\n",
    "        except:\n",
    "            print(\"No more suggested wines\")\n",
    "            continue\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6960a0b-ec16-46aa-a038-1bea79a87a75",
   "metadata": {},
   "source": [
    "### Food-Wine Pairing Tests\n",
    "\n",
    "Questions\n",
    "1. What are the best dishes to serve with a 2020 Puligny-Montrachet Chardonnay?\n",
    "2. I have a bottle of Amarone della Valpolicella — what foods would pair well with it?\n",
    "3. What should I cook for dinner to go with a chilled bottle of Sancerre?\n",
    "4. Can you suggest a full-course meal to go with a vintage Champagne?\n",
    "5. What kind of food works well with a sweet Riesling from Mosel?\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "870ad854-df33-4428-b197-e83425e54850",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Base Models for Reverse Pairings\n",
    "class WinePairing(BaseModel):\n",
    "    \"\"\"Structured metadata filters for querying a wine product database.\"\"\"\n",
    "\n",
    "    wine_name: Optional[str] = Field(\n",
    "        default=None, description=\"Specific wine name mentioned in the query.\"\n",
    "    )\n",
    "    wine_type: Optional[str] = Field(\n",
    "        default=None, description=\"Type of wine (e.g., red, white, rosé, sparkling).\"\n",
    "    )\n",
    "    wine_grapes: Optional[str] = Field(\n",
    "        default=None, description=\"Grape variety or blend (e.g., Merlot, Syrah).\"\n",
    "    )\n",
    "    level_to_drink: Optional[str] = Field(\n",
    "        default=None, description=\"Drinkability status (e.g., 'drink now', 'ageing potential').\"\n",
    "    )\n",
    "    season: Optional[str] = Field(\n",
    "        default=None, description=\"Season the wine is suited for (e.g., 'summer').\"\n",
    "    )\n",
    "    description: Optional[str] = Field(\n",
    "        default=None, description=\"Flavor notes or sensory descriptions.\"\n",
    "    )\n",
    "    occasion: Optional[str] = Field(\n",
    "        default=None, description=\"Occasion suitability (e.g., 'wedding', 'gift').\"\n",
    "    )\n",
    "    body: Optional[str] = Field(\n",
    "        default=None, description=\"Body type (e.g., 'light', 'full-bodied').\"\n",
    "    )\n",
    "    acidity: Optional[str] = Field(\n",
    "        default=None, description=\"Acidity level (e.g., 'crisp', 'low').\"\n",
    "    )\n",
    "    alcohol: Optional[float] = Field(\n",
    "        default=None, description=\"Alcohol content as a percentage (e.g., 13.5).\"\n",
    "    )\n",
    "    fruitiness: Optional[str] = Field(\n",
    "        default=None, description=\"Level of fruitiness (e.g., 'dry', 'juicy').\"\n",
    "    )\n",
    "    minerality: Optional[str] = Field(\n",
    "        default=None, description=\"Presence of mineral notes (e.g., 'chalky').\"\n",
    "    )\n",
    "    sweetness: Optional[str] = Field(\n",
    "        default=None, description=\"Sweetness level (e.g., 'dry', 'semi-sweet').\"\n",
    "    )\n",
    "\n",
    "\n",
    "class FoodMetadata(BaseModel):\n",
    "    \"\"\"Structured metadata filters for querying a food product database.\"\"\"\n",
    "    \n",
    "    dish_name: Optional[str] = Field(\n",
    "        default=None, description=\"Name of the dish (e.g., 'roast duck', 'brie cheese').\"\n",
    "    )\n",
    "    min_price: Optional[float] = Field(\n",
    "        default=None, description=\"Minimum price filter (inclusive).\"\n",
    "    )\n",
    "    max_price: Optional[float] = Field(\n",
    "        default=None, description=\"Maximum price filter (inclusive).\"\n",
    "    )\n",
    "    type: Optional[str] = Field(\n",
    "        default=None, description=\"Food type (e.g., 'fruit', 'pastry', 'vegetarian').\"\n",
    "    )\n",
    "    course: Optional[str] = Field(\n",
    "        default=None, description=\"Course (e.g., 'starter', 'main', 'dessert').\"\n",
    "    )\n",
    "    description: Optional[str] = Field(\n",
    "        default=None, description=\"Description of the dish.\"\n",
    "    )\n",
    "    acidity: Optional[str] = Field(\n",
    "        default=None, description=\"Acidity match or contrast for the dish (e.g., 'low', 'crisp').\"\n",
    "    )\n",
    "    regional_pairing: Optional[str] = Field(\n",
    "        default=None, description=\"Regional or traditional pairing origin (e.g., 'Provence').\"\n",
    "    )\n",
    "    sweetness: Optional[str] = Field(\n",
    "        default=None, description=\"Sweetness level or match (e.g., 'dry', 'sweet').\"\n",
    "    )\n",
    "    wine_pairings: List[WinePairing] = Field(\n",
    "        default=None,\n",
    "        description=\"List of recommended or matching wine pairings.\"\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field_name, field in self.model_fields.items():\n",
    "            val = getattr(self, field_name)\n",
    "            if val is not None and val != field.default:\n",
    "                print(f\"{field_name}: {val}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "22e2e5e6-6731-4ba1-b26a-47cfcdd70f07",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "food_system = \"\"\"You are an expert at converting user questions into structured metadata filters \n",
    "to query a database of food products\n",
    "\n",
    "Your job is to convert a user's question into a JSON object that matches the following schema:\n",
    "            \n",
    "{\n",
    "    \"dish_name\": \"Optional[str]\",                 // Name of the dish (e.g., 'roast duck', 'brie cheese').\n",
    "    \"min_price\": \"Optional[float]\" ,              // Minimum price filter (inclusive).\n",
    "    \"max_price\": \"Optional[float]\" ,              // Maximum price filter (inclusive).\n",
    "    \"type\": \"Optional[str]\" ,                     // Food type (e.g., 'fruit', 'pastry', 'vegetarian').\n",
    "    \"course\": \"Optional[str]\" ,                   // Course (e.g., 'starter', 'main', 'dessert').\n",
    "    \"description\": \"Optional[str]\" ,              // Description of the dish.\n",
    "    \"acidity\": \"Optional[str]\" ,                  // Acidity match or contrast for the dish (e.g., 'low', 'crisp').\n",
    "    \"regional_pairing\": \"Optional[str]\" ,         // Regional or traditional pairing origin (e.g., 'Provence').\n",
    "    \"sweetness\": \"Optional[str]\" ,                // Sweetness level or match (e.g., 'dry', 'sweet').\n",
    "\n",
    "    \"wine_pairings\": [                            // Optional: List of recommended food matches\n",
    "    {\n",
    "          \"wine_name\": \"Optional[str]\",              // Specific wine name mentioned\n",
    "          \"wine_type\": \"Optional[str]\",              // Type of wine (e.g. red, white, rosé, sparkling)\n",
    "          \"wine_grapes\": \"Optional[str]\",            // Grape variety or blend (e.g. Merlot, Syrah)\n",
    "          \"level_to_drink\": \"Optional[str]\",         // Drinkability status (e.g. \"drink now\", \"ageing potential\")\n",
    "          \"season\": \"Optional[str]\",                 // Season the wine is suited for (e.g. \"summer\", \"winter\")        \n",
    "          \"description\": \"Optional[str]\",            // Flavor notes or sensory descriptions\n",
    "          \"occasion\": \"Optional[str]\",               // Occasion suitability (e.g. \"wedding\", \"gift\", \"everyday\")\n",
    "          \"body\": \"Optional[str]\",                   // Body type (e.g. \"light\", \"full-bodied\")\n",
    "          \"acidity\": \"Optional[str]\",                // Acidity level (e.g. \"crisp\", \"low\")\n",
    "          \"alcohol\": \"Optional[float]\",              // Alcohol content (as percentage, e.g. 13.5)\n",
    "          \"fruitiness\": \"Optional[str]\",             // Level of fruitiness (e.g. \"dry\", \"juicy\")\n",
    "          \"minerality\": \"Optional[str]\",             // Presence of mineral notes (e.g. \"chalky\", \"flinty\")\n",
    "          \"sweetness\": \"Optional[str]\",              // Sweetness level (e.g. \"dry\", \"semi-sweet\", \"sweet\")\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Only include optional fields if they are explicitly mentioned in the user's query. \n",
    "Return the result as **pure JSON only**, with no code block, no Markdown, and no explanation.\n",
    "\"\"\"\n",
    "\n",
    "food_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", food_system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d0884cc-3bf6-4d20-bea7-4c5d35ac222f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "FOOD_FIELD_MAP = {\n",
    "    \"dish_name\": \"Product Name\",\n",
    "    \"price\": \"Price\",\n",
    "    \"type\": \"Food Type\",\n",
    "    \"course\": \"Course\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56fefc-8bfd-4a7c-b9a6-b951adc0b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def timed(log_times):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end = time.time()\n",
    "            log_times[func.__name__] = round(end - start, 4)\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "log_times = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "feb3f583-ede1-40dc-b2ec-0a22b33fed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed(log_times)\n",
    "def food_query_analyzer(question: str) -> FoodMetadata:\n",
    "    \"\"\"\n",
    "    Converts a natural language question into a structured query\n",
    "    using a DeepSeek LLM backend.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=food_system),\n",
    "        HumanMessage(content=question)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    print(response.content)\n",
    "    parsed_dict = extract_json_from_llm_output(response.content)\n",
    "    return FoodMetadata(**parsed_dict)\n",
    "\n",
    "@timed(log_times)\n",
    "def filter_food(data, model_instance, field_map, max_results=20):\n",
    "    \"\"\"\n",
    "    Given metadata filters constructed from user query using LLM,\n",
    "    returns first n wine profiles that match. \n",
    "    \"\"\"\n",
    "    filters = model_instance.model_dump(exclude_none=True)\n",
    "    results = []\n",
    "\n",
    "    for food in data:\n",
    "        match = True\n",
    "        for key, value in filters.items():\n",
    "            if key not in field_map:\n",
    "                continue\n",
    "                \n",
    "            elif key in (\"min_price\", \"max_price\"):\n",
    "                price_field = field_map[key]\n",
    "                try:\n",
    "                    food_price = float(food.get(price_field,0))\n",
    "                except ValueError:\n",
    "                    match = False\n",
    "                    break\n",
    "                if key == \"min_price\" and food_price < value:\n",
    "                    match = False\n",
    "                    break\n",
    "                if key == \"max_price\" and food_price > value:\n",
    "                    match = False\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                field = field_map.get(key)\n",
    "                if field not in food:\n",
    "                    match = False\n",
    "                    # print(f\"[MISSING] Field '{field}' missing in wine\")\n",
    "                    break\n",
    "                food_val = normalize(food[field])\n",
    "                query_val = normalize(value)\n",
    "                if query_val not in food_val:\n",
    "                    # print(f\"[FAIL] {key}: '{query_val}' not in '{wine_val}' (wine: {wine.get('Product Name')})\")\n",
    "                    match = False\n",
    "                    break\n",
    "\n",
    "        if match:\n",
    "            results.append(food)\n",
    "            if len(results)== max_results:\n",
    "                break\n",
    "\n",
    "    return results\n",
    "\n",
    "@timed(log_times)\n",
    "def create_taste_profile(parsed_query):\n",
    "    \"\"\"\n",
    "    Enriches wine taste profile from user query using LLM.\n",
    "    \"\"\"\n",
    "    data = parsed_query.model_dump(exclude_none=True)\n",
    "\n",
    "    # Check if food_pairing exists and is not empty\n",
    "    wine_pairings = data.get(\"wine_pairings\")\n",
    "    pairing_descriptions = []\n",
    "    if wine_pairings:\n",
    "        # Extract raw attributes\n",
    "        for pairing in wine_pairings:\n",
    "            if isinstance(pairing, dict):\n",
    "                entries = [f\"{key}: {value}\" for key, value in pairing.items() if value]\n",
    "                block = \"\\n\".join(entries)\n",
    "                pairing_descriptions.append(block)\n",
    "            else:\n",
    "                try:\n",
    "                    entries = [f\"{key}: {getattr(pairing, key)}\" for key in pairing.__fields__ if getattr(pairing, key)]\n",
    "                    block = \"\\n\".join(entries)\n",
    "                    pairing_descriptions.append(block)\n",
    "                except:\n",
    "                    continue\n",
    "        raw_input = \"\\n\\n---\\n\\n\".join(pairing_descriptions)\n",
    "\n",
    "        # RAG\n",
    "        response = qa_chain.invoke(f\"Create a taste profile based on the following context: {raw_input}\")\n",
    "        return response[\"result\"]\n",
    "    return None                       \n",
    "\n",
    "@timed(log_times)\n",
    "def generate_recommendations(filtered, profile, top_k=5):\n",
    "    \"\"\"\n",
    "    Performs similarity search between embeddings of (a) each of the filtered wines\n",
    "    and (b) enriched taste profile from query, if present otherwise randomly \n",
    "    sample 3 wines (cheapest -> middle -> most expensive)\n",
    "    \"\"\"\n",
    "    if len(filtered) == 0:\n",
    "        return None\n",
    "        \n",
    "    if len(filtered) < top_k:\n",
    "        return filtered\n",
    "        \n",
    "    if profile:\n",
    "        # print(\"yay\")\n",
    "        embedded_food = []\n",
    "        for food in filtered:\n",
    "            # print(wine)\n",
    "            content = \"\\n\".join(f\"{key}: {value}\" for key, value in food.items() if value)\n",
    "            food_embed = embd.embed_query(content)\n",
    "            embedded_food.append((food, food_embed))\n",
    "\n",
    "        profile_embed = embd.embed_query(profile)\n",
    "        scored_embed = [(food, cosine_similarity(profile_embed, food_embed)) for food, food_embed in embedded_food]\n",
    "        top_food = sorted(scored_embed, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        return [food for food, _ in top_food]\n",
    "\n",
    "    else:\n",
    "        # print(\"nooo\")\n",
    "        sorted_wines = sorted(filtered, key=lambda x: float(x.WS_Retail_Price))\n",
    "        n = len(sorted_wines)\n",
    "        if n == 0:\n",
    "            return []\n",
    "        step = max(1, n // top_k)\n",
    "        sampled = [random.choice(sorted_wines[i:i+step]) for i in range(0, n, step)][:top_k]\n",
    "\n",
    "        return sampled \n",
    "\n",
    "@timed(log_times)\n",
    "def ask_food_ai(question, data, field_map):\n",
    "    start_main = time.time()\n",
    "    parsed_query = food_query_analyzer(question)\n",
    "    # parsed_query.pretty_print()\n",
    "    filtered = filter_wines(data=data, model_instance=parsed_query, field_map=field_map)\n",
    "    # print(\"\\n\\nFiltered:\", filtered)\n",
    "    profile = create_taste_profile(parsed_query)\n",
    "    # print(\"\\n\\nProfile:\", profile)\n",
    "    recommendations = generate_recommendations(filtered, profile)\n",
    "    # print(recommendations)\n",
    "    \n",
    "    total_time = round(time.time() - start_main, 4)\n",
    "    # Print timing summary\n",
    "    print(\"Timing summary:\")\n",
    "    for name, duration in log_times.items():\n",
    "        print(f\"  {name}: {duration} seconds\")\n",
    "    print(f\"  Total time: {total_time} seconds\")\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "da58e8eb-2e3d-49d1-b990-bf644be4db1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the best dishes to serve with a 2020 Puligny-Montrachet Chardonnay?\n",
      "Timing summary:\n",
      "  query_analyzer: 11.4839 seconds\n",
      "  filter_wines: 0.0 seconds\n",
      "  create_taste_profile: 46.7189 seconds\n",
      "  generate_recommendations: 1.7185 seconds\n",
      "  food_query_analyzer: 7.6931 seconds\n",
      "  Total time: 56.1305 seconds\n",
      "Food 1: Alsatian Apple Tart with Caramelized Pecans\n",
      "Food 2: Bacalhau à Brás\n",
      "Food 3: Apple and Brie Tartlets with Caramelized Onions\n",
      "Food 4: Alsatian Bacon and Onion Tart\n",
      "Food 5: Baked Camembert with Fig Jam\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "\"What are the best dishes to serve with a 2020 Puligny-Montrachet Chardonnay?\"\n",
    "]\n",
    "\n",
    "for i in range(len(questions)):\n",
    "# for i in range(2, 5, 2):\n",
    "    print(questions[i])  # Correct variable name\n",
    "    recommendation = ask_food_ai(question=questions[i], data = food_json, field_map = FOOD_FIELD_MAP)\n",
    "    if recommendation == None:\n",
    "        print(\"No recommendation found\")\n",
    "        continue\n",
    "\n",
    "    for j in range(5):\n",
    "        try:\n",
    "            print(f\"Food {j+1}:\", recommendation[j]['Product Name'])\n",
    "        except:\n",
    "            print(\"No more suggested food\")\n",
    "            continue\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "17f1eb70-3ef6-4828-b4d2-11713037e441",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mWhat are the best dishes to serve with a 2020 Puligny-Montrachet Chardonnay?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m start_main = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m parsed_query = \u001b[43mfood_query_analyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m end_time = time.time()\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtime:\u001b[39m\u001b[33m\"\u001b[39m, {\u001b[38;5;28mround\u001b[39m(end_time - start_main, \u001b[32m4\u001b[39m)})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtimed.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m      9\u001b[39m     start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     end = time.time()\n\u001b[32m     12\u001b[39m     log_times[func.\u001b[34m__name__\u001b[39m] = \u001b[38;5;28mround\u001b[39m(end - start, \u001b[32m4\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mfood_query_analyzer\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mConverts a natural language question into a structured query\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03musing a DeepSeek LLM backend.\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m messages = [\n\u001b[32m      8\u001b[39m     SystemMessage(content=food_system),\n\u001b[32m      9\u001b[39m     HumanMessage(content=question)\n\u001b[32m     10\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.content)\n\u001b[32m     14\u001b[39m parsed_dict = extract_json_from_llm_output(response.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\rag-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\rag-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\rag-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\rag-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\rag-env\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1071\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m   1070\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1071\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\rag-env\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\rag-env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\rag-env\\Lib\\site-packages\\openai\\_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\rag-env\\Lib\\site-packages\\openai\\_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "question = \"What are the best dishes to serve with a 2020 Puligny-Montrachet Chardonnay?\"\n",
    "start_main = time.time()\n",
    "parsed_query = food_query_analyzer(question)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"time:\", {round(end_time - start_main, 4)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "rag-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
